 <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="research">
        <div class="my-auto">
          <h2 class="mb-5">Research</h2>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">How should we call it? - Introducing the PhotoBook Conversation Task and Dataset for
Training Natural Referring Expression Generation in Artificial Dialogue Agents (Master's Thesis)</h3>
              <div class="subheading mb-3">Supervision: <a href="https://staff.fnwi.uva.nl/r.fernandezrovira/" target="_blank">Dr. Raquel Fernández</a> and <a href="https://staff.fnwi.uva.nl/e.bruni/" target="_blank">Dr. Elia Bruni</a></div>
              <p>The past few years have seen an immense interest in developing and training computational agents
for visually-grounded dialogue, the task of using natural language to communicate about visual
input. While the resulting dialogue agents often already achieve reasonable performance on their
respective task, none of the models can produce consistent and efficient outputs during a multi-turn
conversation. We argue that this is primarily due to the fact that they cannot properly utilise the
dialogue history. Human interlocutors on the other hand are shown to collaboratively establish a
shared repository of mutual information during a conversation. This common ground then is used to
optimise understanding and communication efficiency. We therefore propose that implementing a
similar representation of dialogue context for computational dialogue agents is a pivotal next step in
improving the quality of their dialogue output.</p><p>
One of the main reasons why current research seems to eschew modelling common ground is that
common ground is a conversation model concept that cannot be assessed directly in actual
conversations. In order to address this problem, we propose to first investigate the generation of
referring expressions: Being an indirect representation of a referent object, they too are not
absolute, but conventions established with a specific conversation partner - based on the common
ground established so far. By tracking the development of referring expressions during a
conversation we therefore obtain a proxy of the underlying processes in the emerging common
ground. </p><p>
In order to develop an artificial dialogue agent that can utilise the conversation's common ground,
we propose to implement a data-driven, modular agent architecture in an end-to-end training
framework. With this setup, the dialogue agent is expected to learn the correct usage of referring
expressions from recorded dialogue data directly and can be evaluated on downstream task
performance. Opting for this approach however requires a large amount of dedicated dialogue
training data that has never been collected before. To initiate this new track in dialogue modelling
research, we therefore introduce a novel conversation task called the PhotoBook task that can be
used to collect rich, human-human dialogue data for extended, goal-oriented conversations. We use
the PhotoBook task to record more than 2,500 dialogues stemming from over 1,500 unique
participants on crowd-sourcing platform Amazon Mechanical Turk (AMT). The resulting data
contains a total of over 160k utterances, 130k actions and spans a vocabulary of close to 12k
unique tokens. An extensive analysis of the data validates that the recorded conversations closely
resemble the dialogue characteristics observed in natural human-human conversations. We
therefore argue that this data provides a pivotal new repository to be used in further research which
has the potential to significantly improve the dialogue output consistency, efficiency and naturalness
of artificial dialogue agents.
               </p>
               <p><b>My thesis is available as a download from the university's server <a href="https://esc.fnwi.uva.nl/thesis/centraal/files/f738860517.pdf" target="_blank">here</a>.</b></p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">Ongoing</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Topic Segmentation in Spoken Dialogue through Convergence in Utterance Complexity
</h3>
              <div class="subheading mb-3">Supervision: <a href="https://staff.fnwi.uva.nl/r.fernandezrovira/" target="_blank">Dr. Raquel Fernández</a></div>
              <p>In 2018, Xu and Reitter introduced a novel,
information-theoretic view of dialogue, in which
they proposed modeling a conversation between
two interlocutors as a two-way communication
system. In such a system the information flow follows a number of general principles. One of those
principles that is assumed to hold in dialogue as well is the
Uniform Information Density hypothesis (UID). The UID hypothesis states
that a communication system as a whole has the
tendency to distribute data in such a way that the
density of information remains constant.
</p><p>
In two-party dialogue both interlocutors are equal
parts of the communication system. This means
that they are jointly responsible for the level of
information density at every moment of the conversation. In order to ensure the validity of the
UID hypothesis, the two speakers must therefore
have an agreement in an implicit sense about their
contribution to the conversation. 
Xu and Reitter propose that speakers take on certain roles during a conversation: One leads the conversation by
steering the ongoing topic, while the other follows
along. These roles can switch during a conversation and rather than steering turn-taking behavior,
they describe a higher-level segmentation of a conversation into topics.
</p><p>
In this research we investigate whether we can detect
the boundaries of these conversation segments,
formally referred to as topic shifts, based on the speaker’s contribution to the conversation alone. To this end we extract a number of simple syntactical features that have been
shown to correlate well with the amount of information transmitted and build
a simple prediction model based on these features.
</p><p>
While we had to conclude that this simple approach does not yield a model expressive enough
to correctly predict topic shifts produced by more
involved methods, we beleive that it nonetheless
produces coherent and intuitively sound topic segments even for noisy dialogue transcripts. As a next step, we will investigate different methods to validate this claim. 
</p>
<b>You can view our preliminary results <a href="/data/csp_project_report.pdf" target="_blank">here.</a> </b></p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">2018</span>
            </div>
          </div>


          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Automatic Timeline Summarization from Non-Curated News Streams</h3>
              <div class="subheading mb-3">Supervision: <a href="http://nickvosk.github.io/" target="_blank"> Nikos Voskarides </a> </div>
              <p>The continuously increasing amount of online news articles requires new ways of filtering relevant information into a human-digestible form. Recently, research
                  has focused on providing such selections by generating timelines for known entities through extending and extracting information from Knowledge Graphs. 
                  Contrasting this approach, we propose a new method to generate an entity timeline based directly on a non-curated, unstructured set of news items so as to allow this approach to be extended to long-tail entities. </p><p>
                  In this research, Wikipedia pages of entities are seen as a gold-label timeline consisting of information cited from news-worthy articles, while other news articles about those entities that are not cited are treated as negative examples. To learn what makes an article news-worthy, we take a supervised approach based on a set of 28 handcrafted features. 
                  </p><p>
                  One of our main contributions is a novel, larger dataset for this task, covering 379 unique entities and containing 13146 news articles with an equal
                  distribution of positive and negative examples per entity. Using this dataset we obtain a basic classification accuracy of 68.9% for deciding whether an unseen news article contains relevant information about a given entity. As a baseline method of evaluation, the top article predictions per entity are then summarized and concatenated to generate a dummy Wikipedia entries which we compare to the original ones. As no standardized, gold-label evaluation methods were developed yet, we also propose an A/B testing method for a more qualitative performance estimate.
                  </p><p>
                  <b>You can read the project paper <a target="_blank" href="/data/project_ai_report.pdf">here.</a> </b></p>
                </p>
              </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">2017</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Modeling Distributed Cybernetic Management for Resource Based Economies (Bachelor's Thesis) </h3>
              <div class="subheading mb-3">Supervision: <a href="https://staff.fnwi.uva.nl/r.valenti/" target="_blank"> Dr. Roberto Valenti</a></div>
              <p> In the early 1970's, AI once before was THE big thing that would revolutionize the world as they knew it. Many great researchers were optimistic that artificial systems with general intelligence were within grasp - and big plans were made to apply such systems to solve real-world problems. Among the most notorious ones: 1971’s  CyberSyn project of British economist Stafford Beer - which came to an abrupt and violent end just two years later. </p><p>
              The context: 1970 Chile elected its first socialist president, Salvador Allende, which in turn appointed Fernando Flores, a young scientist devoted to the study of operations
              research and scholar of Beer’s work on the subject of management cybernetics to be the General Technical Manager of the new-found state development agency. In that function, Flores invited Beer to design and implement a cybernetic system to automatize the administration of the entire Chilean economy. An ambitious project that after taking first steps was cut short by a military coup in 1973. </p><p>
              In this research we aim to investigate whether the simple cybernetic approach proposed by Stafford and his colleagues could have been sufficient to manage something as complex as the Chilean state economy. We do so by modeling a simplified economic setting governed by CyberSyn’s management principles and analyze the model’s performance under a range
              of different parameter settings. The results of these initial experiments suggest that the model indeed exhibits emerging self-sustainability and lead to the
              conclusion that CyberSyn’s approach might have been principally feasible. </p><p> <b>You can find my Bachelor's Thesis <a target="_blank" href="https://esc.fnwi.uva.nl/thesis/centraal/files/f1930828629.pdf">here.</a> </b></p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">2015</span>
            </div>
          </div>


          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Philosophical Essay about Dennett’s Answer to the Objection of Original Intentionality in Artifacts</h3>
              <div class="subheading mb-3">Supervision: <a href="hhttps://staff.fnwi.uva.nl/r.a.m.vanrooij/" target="_blank"> Prof. Dr. Robert van Rooij</a></div>
              <p>  Many critics of AI argue that intentionality in computers - or any other artifact
for that matter - can never be more than derivative. With the words of John
Haugeland, their “tokens only have meaning because we give it to them” and
consequently, “they only mean what we say that they do”. Contemporary philosopher Daniel Dennett however claims
that “there is no principled (theoretically motivated) way to distinguish ‘original’ intentionality from ‘derived’ intentionality.” On the basis of this idea, he developed a three-stage model to explain the assignment of intentionality and
refute the objection of derived intentionality in artifacts.
</p><p> 
In this essay, we analyze Dennett’s model and answer the question
How does Dennett’s elaborated model of the intentional stance answer Haugeland’s objection that intentionality in artifacts cannot be
original? </p><p> <b>You can read the essay <a href="/data/philosophy_essay_dennett.pdf" target="_blank">here.</a> </b></p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">2015</span>
            </div>
          </div>


           



        </div>
      </section>
